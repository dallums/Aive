{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aive Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells are meant to be run in order. During the output of either video (face or human), press enter to stop and end the analysis. Sometimes it's buggy and you need to go Kernel -> Restart & Clear Output to kill the video popup.\n",
    "\n",
    "Install packages manually in a venv, or used the anaconda3 kernel for the notebook for best results. The YouTube video needs to be downloaded as an mp4 in this case as well.\n",
    "\n",
    "I use OpenCV, which seems pretty standard, and tune paramters based on some initial high level advice here: https://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"MISS DIOR The new Eau de Parfum.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed heavily from: https://www.analyticsvidhya.com/blog/2018/12/introduction-face-detection-video-deep-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB   \n",
    "    # color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "    \n",
    "    # Find all the faces in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    \n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, \n",
    "                      (left, top), \n",
    "                      (right, bottom), \n",
    "                      (0, 0, 255), \n",
    "                      2)\n",
    "        \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video-Faces', \n",
    "               frame)\n",
    "\n",
    "    # Wait for Enter key to stop\n",
    "    if cv2.waitKey(25) == 13:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowed heavily from: https://thedatafrog.com/en/articles/human-detection-video/#:~:text=OpenCV%20features%20an%20implementation%20for,work%20well%20in%20other%20cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    try:\n",
    "        # resizing for faster detection\n",
    "        frame = cv2.resize(frame, \n",
    "                           (640, 480))\n",
    "        # using a greyscale picture, also for faster detection\n",
    "        gray = cv2.cvtColor(frame, \n",
    "                            cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # detect people in the image\n",
    "        # returns the bounding boxes for the detected objects\n",
    "        boxes, weights = hog.detectMultiScale(frame, \n",
    "                                              winStride=(4,4),\n",
    "                                              scale=1.01)\n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "        for (xA, yA, xB, yB) in boxes:\n",
    "            # display the detected boxes in the colour picture\n",
    "            cv2.rectangle(frame, \n",
    "                          (xA, yA), \n",
    "                          (xB, yB),\n",
    "                          (0, 255, 0), \n",
    "                          2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Video-Humans',\n",
    "                   frame)\n",
    "        if cv2.waitKey(25) == 13:\n",
    "            break\n",
    "            \n",
    "    except:\n",
    "        #print('Bad Frame')\n",
    "        pass\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The face identifier works better out of the box with minimal tuning than the huamn identifier, but both still need a lot of work. It's interesting that in so few lines of code, we can get a MVP up and running, but as a next step, I'd:\n",
    "\n",
    "* Tune the hypermarameters better;\n",
    "* Implement https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "* Train the model on similar videos and pictures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
